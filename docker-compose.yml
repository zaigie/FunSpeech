services:
  funspeech:
    image: docker.cnb.cool/nexa/funspeech:latest
    container_name: funspeech
    ports:
      - "8000:8000"
    volumes:
      # 模型缓存目录（ModelScope默认缓存路径）
      - ~/.cache/modelscope:/root/.cache/modelscope
      # 临时文件目录（ASR和TTS共用）
      - ./temp:/app/temp
      # 数据持久化目录（数据库等）
      - ./data:/app/data
      # 日志目录
      - ./logs:/app/logs
      # 零样本音色文件目录（用户数据）
      - ./voices:/app/voices
    environment:
      # 服务器配置
      - DEBUG=false
      - LOG_LEVEL=INFO

      # GPU配置（统一格式）
      # 支持: "" (自动检测), "cpu" (使用CPU), "0" (单卡), "0,1,2" (多卡负载均衡)
      # - TTS_GPUS=0          # TTS使用的GPU（默认自动检测）
      # - ASR_GPUS=0          # ASR使用的GPU（默认自动检测）

      # ASR 模型按需加载配置（可选，默认为 all）
      # - ASR_MODEL_MODE=all          # 加载默认模型的离线和实时版本（默认）
      # - ASR_MODEL_MODE=offline      # 仅加载离线模型
      # - ASR_MODEL_MODE=realtime     # 仅加载实时流式模型

      # ASR 自定义模型预加载（可选，默认为空）
      # 配置后在启动时自动加载指定的自定义模型，避免首次调用时的等待
      # 可用模型: sensevoice-small, dolphin-small
      # - AUTO_LOAD_CUSTOM_ASR_MODELS=sensevoice-small              # 单个模型
      # - AUTO_LOAD_CUSTOM_ASR_MODELS=sensevoice-small,dolphin-small  # 多个模型（逗号分隔）

      # ASR 实时标点符号模型（可选，默认为 false）
      # - ASR_ENABLE_REALTIME_PUNC=true  # 启用实时标点符号模型（用于流式识别中间结果）

      # 流式ASR远场声音过滤配置（可选，默认已启用）
      # 自动过滤远场声音和环境音，减少误触发，详见 docs/nearfield_filter.md
      # - ASR_ENABLE_NEARFIELD_FILTER=true              # 总开关（默认启用）
      # - ASR_NEARFIELD_RMS_THRESHOLD=0.01              # RMS能量阈值（默认宽松模式）
      # - ASR_NEARFIELD_FILTER_LOG_ENABLED=true         # 调试日志（默认启用，调优后可关闭）

      # TTS 模型按需加载配置（可选，默认为 all）
      # - TTS_MODEL_MODE=all          # 加载所有模型（默认）
      # - TTS_MODEL_MODE=sft          # 仅加载SFT模型，用于预设音色
      # - TTS_MODEL_MODE=clone        # 仅加载零样本克隆模型，用于音色克隆

      # 克隆模型版本选择（可选，默认为 cosyvoice3）
      # - CLONE_MODEL_VERSION=cosyvoice3   # 使用 Fun-CosyVoice3-0.5B-2512（默认）
      # - CLONE_MODEL_VERSION=cosyvoice2   # 使用 CosyVoice2-0.5B
      # - COSYVOICE3_MODEL_ID=FunAudioLLM/Fun-CosyVoice3-0.5B-2512  # CosyVoice3 模型 ID（可选）

      # 推理线程池配置（可选，默认为 max(4, CPU核心数)）
      # 用于实现多路并发推理，每个请求的模型推理在独���线程中执行
      # - INFERENCE_THREAD_POOL_SIZE=8

      # Worker 进程数配置（可选，默认为 1）
      # 多 Worker 可实现真正的并行计算，但每个 Worker 都会加载独立的模型副本
      # 注意：会成倍增加内存/显存占用，请确保资源充足
      # - WORKERS=2

      # 鉴权配置（可选，不设置则鉴权为可选）
      # - APPTOKEN=your_secret_token
      # - APPKEY=your_app_key
    restart: unless-stopped
    
    # GPU支持（如果需要，取消注释以下配置并将image替换为docker.cnb.cool/nexa/funspeech:gpu-latest）
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           count: all
    #           capabilities: [gpu]